# How to Build Neural Networks

<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/neural_networks.png?raw=true">
</p>

When I think of neural networks, I imagine something similar to the image depicted above. This is the first article in a series of guides that will teach you how to build, train, and test neural networks using a concrete example in C#. In Article 2, we'll modify the neural network to leverage multi-core processors and explore why this leads to non-reproducibility and varying results across multiple training sessions. In Article 3, we will explore both effective and ineffective coding practices, as well as optimization techniques such as SIMD, to achieve the best possible results for our neural network. Article 4 focuses on optimizing the neural network training process and evaluating the performance achieved, with a focus on achieving the best possible results. 

1. https://github.com/grensen/how_to_build
2. https://github.com/grensen/multi-core
3. https://github.com/grensen/good_vs_bad_code
4. https://github.com/grensen/how_to_train

## 105 Miss Predictions
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/incorrect_105.png?raw=true">
</p>

For this project, the MNIST handwritten digits dataset was utilized, consisting of 60,000 training images and 10,000 test images. Each image is a grayscale 28 by 28-pixel image with pixel values ranging from 0 to 255, representing a digit from 0 to 9.

The neural network achieved an impressive accuracy of 98.95% in classifying these digits. However, upon further analysis, it was discovered that there were 105 incorrect predictions out of the 10,000 test images. These misclassifications highlight the difficulties in accurately classifying handwritten digits, despite the high overall accuracy of the neural network

## Target Learning: Understanding the Basics of Supervised Learning
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/naive_learning.png?raw=true">
</p>

The image demonstrates a simple example of supervised learning in machine learning. It shows a single input being multiplied by a weight, which is then added to an output. The gradient is calculated as the difference between the target value and the output value, and the weight is updated using this gradient and the input value. This process is repeated multiple times to adjust the weight to better fit the desired output.

## Neurons + Weights = Forward Pass
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/init_neurons_weights_indices.png?raw=true">
</p>

A neural network is composed of layers of interconnected nodes, called neurons. Each neuron receives input from the previous layer, performs a calculation using its internal weight values, and outputs the result to the next layer. During the forward pass, the input data is fed into the first layer of neurons, and the outputs of each layer are propagated forward until the final layer produces the network's prediction. The weights of the neurons are learned through a process called training, where the network adjusts its weights to minimize the difference between its predicted output and the true output.

## Backward Pass = Gradients + Deltas
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/init_gradients_deltas_indices.png?raw=true">
</p>

This image illustrates the architecture of a neural network during the forward pass. At each layer of the network, the input data is transformed using weights and bias parameters associated with each neuron. The transformed data is then passed through activation functions that help to introduce non-linearity into the network, leading to more complex representations of the input data. The result of the forward pass is a prediction generated by the output layer of the network.

## The Process For Every Weight
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/neural_network_process.png?raw=true">
</p>

This image depicts the backward pass process in a neural network, which is used to update the network's weights to improve its accuracy. During the backward pass, the gradients of the loss function with respect to the network's parameters are computed using the chain rule of calculus. These gradients are then used to update the weights in the opposite direction of the gradient, with the goal of reducing the loss function. The size of the update is controlled by a learning rate hyperparameter, and the process is repeated for each batch of training data until the network converges to a good solution.

## Batch 1
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/batch1.png?raw=true">
</p>

## Batch 2
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/batch2.png?raw=true">
</p>

## Weight Update
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/Update_batch.png?raw=true">
</p>

## New Batch
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/new_batch.png?raw=true">
</p>

## Activations: Hidden = ReLU, Output = Softmax
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/neural_network_activations.png?raw=true">
</p>

## Understand ReLU Activations With DESMOS
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/ReLU_ji.png?raw=true">
</p>

## Understand Softmax Activations 
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/exp_ji.png?raw=true">
</p>

## Latex
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/perceptron_vs_layer.png?raw=true">
</p>

## Feed Forward Perceptron-Wise
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/ff_perceptron-wise.gif?raw=true">
</p>

## Feed Forward Layer-Wise
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/ff_layer-wise.gif?raw=true">
</p>

## Init Your Picture
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/init_with_input.png?raw=true">
</p>

## Trained With 50% Pseudo Probability For One Problem
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/trained_with_input2.png?raw=true">
</p>


## High Level Overview
~~~
1. Preprocess the data.
2. Define the model.
3. Train the model.
4. Evaluate the model.
~~~

## Code Intuition
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/network_intuition.png?raw=true">
</p>

## High Level Code
~~~cs
// https://github.com/grensen/how_to_build/
#if DEBUG
    System.Console.WriteLine("Debug mode is on, switch to Release mode");
#endif 
System.Action<string> print = System.Console.WriteLine;

print("\nBegin how to build neural networks demo\n");

// get data
AutoData d = new(@"C:\mnist\");

// define neural network 
int[] network       = { 784, 100, 100, 10 };
var LEARNINGRATE    = 0.0005f;
var MOMENTUM        = 0.67f;
var EPOCHS          = 50;
var BATCHSIZE       = 800;
var FACTOR          = 0.99f;

RunDemo(d, network, LEARNINGRATE, MOMENTUM, FACTOR, EPOCHS, BATCHSIZE);

print("\nEnd how to build neural networks demo");

static void RunDemo(AutoData d, int[] NET, float LR, float MOM, float FACTOR, int EPOCHS, int BATCHSIZE){}
static float RunNet(AutoData d, Net neural, int len, float LR, float MOM, float FACTOR, int EPOCHS, int BATCHSIZE){}
static bool EvalAndTrain(int x, byte target, byte[] samples, Net neural, float[] delta){}
static bool EvalTest(int x, byte label, byte[] samples, Net neural){}
Eval(int x, byte[] samples, Net neural, float[] neuron){}
struct Net{}
struct AutoData{}
class Erratic{}
~~~

## DALL-E 2 Neural Networks
<p align="center">
  <img src="https://github.com/grensen/how_to_build/blob/main/figures/DALL_E_neural_networks.png?raw=true">
</p>
